{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runtime Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For conver yolov5 model to trt plan (**yolov5s-2021-07-28.engine**) and build **libmyplugins.so** for your system see instruction: https://github.com/wang-xinyu/tensorrtx/tree/master/yolov5#how-to-run-yolov5s-as-example\n",
    "\n",
    "Based on this instruction, we wrote a small shell script.\n",
    "\n",
    "```bash\n",
    "cd ../inference/convertors/yolo2tensorrt/bin/\n",
    "./yolov5_tensorrt.sh\n",
    "```\n",
    "Before runing this script, please download install additionaly **opencv-contrib** and **pycuda**\n",
    "\n",
    "   * Copy the resulting **libmyplugins.so** to the folder from which you will run this notebook\n",
    "   * Replace the data model **data/models/Detector/yolov5engine/yolov5s-2021-07-28.engine** with the one you just got from running the script\n",
    "\n",
    "Convert options and ocr\n",
    "* options (inference/convertors/options2tensorrt/convert_numberplate_options_to_onnx.py)\n",
    "* ocrs (inference/convertors/ocr2tensorrt/convert_ocr_to_onnx.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "nomeroff_net_dir = os.path.join(current_dir, \"../../../\")\n",
    "sys.path.append(nomeroff_net_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "import pycuda.autoinit\n",
    "\n",
    "from nomeroff_net.tools import unzip\n",
    "from nomeroff_net.pipelines.number_plate_detection_and_reading_trt_runtime import NumberPlateDetectionAndReadingTrtRuntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOv5 ðŸš€ v6.1-240-g6dd6aea Python-3.8.12 torch-1.11.0a0+b6df043 CUDA:0 (NVIDIA GeForce RTX 2080 Ti, 11020MiB)\n",
      "\n",
      "Loading /var/www/nomeroff-net/examples/ju/benchmark/../../../data/model_repository/yolov5s/1/model.engine for TensorRT inference...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05/31/2022-13:22:51] [TRT] [I] [MemUsageChange] Init CUDA: CPU +313, GPU +0, now: CPU 1705, GPU 6826 (MiB)\n",
      "[05/31/2022-13:22:51] [TRT] [I] Loaded engine size: 43 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05/31/2022-13:22:51] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +509, GPU +222, now: CPU 2267, GPU 7094 (MiB)\n",
      "[05/31/2022-13:22:51] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +115, GPU +52, now: CPU 2382, GPU 7146 (MiB)\n",
      "[05/31/2022-13:22:51] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +42, now: CPU 0, GPU 42 (MiB)\n",
      "[05/31/2022-13:22:51] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +10, now: CPU 2338, GPU 7138 (MiB)\n",
      "[05/31/2022-13:22:51] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +0, GPU +8, now: CPU 2338, GPU 7146 (MiB)\n",
      "[05/31/2022-13:22:51] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +69, now: CPU 0, GPU 111 (MiB)\n",
      "[05/31/2022-13:22:51] [TRT] [I] The logger passed into createInferRuntime differs from one already provided for an existing builder, runtime, or refitter. Uses of the global logger, returned by nvinfer1::getLogger(), will return the existing value.\n",
      "\n",
      "[05/31/2022-13:22:51] [TRT] [I] [MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 2346, GPU 7218 (MiB)\n",
      "[05/31/2022-13:22:51] [TRT] [I] Loaded engine size: 93 MiB\n",
      "[05/31/2022-13:22:51] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +1, GPU +8, now: CPU 2440, GPU 7320 (MiB)\n",
      "[05/31/2022-13:22:51] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +0, GPU +10, now: CPU 2440, GPU 7330 (MiB)\n",
      "[05/31/2022-13:22:51] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +93, now: CPU 0, GPU 204 (MiB)\n",
      "[05/31/2022-13:22:51] [TRT] [I] The logger passed into createInferRuntime differs from one already provided for an existing builder, runtime, or refitter. Uses of the global logger, returned by nvinfer1::getLogger(), will return the existing value.\n",
      "\n",
      "[05/31/2022-13:22:51] [TRT] [I] [MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 2346, GPU 7314 (MiB)\n",
      "[05/31/2022-13:22:51] [TRT] [I] Loaded engine size: 15 MiB\n",
      "[05/31/2022-13:22:51] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 2363, GPU 7338 (MiB)\n",
      "[05/31/2022-13:22:51] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +0, GPU +8, now: CPU 2363, GPU 7346 (MiB)\n",
      "[05/31/2022-13:22:51] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +15, now: CPU 0, GPU 219 (MiB)\n",
      "[05/31/2022-13:22:51] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 2347, GPU 7338 (MiB)\n",
      "[05/31/2022-13:22:51] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +0, GPU +8, now: CPU 2347, GPU 7346 (MiB)\n",
      "[05/31/2022-13:22:51] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +3, now: CPU 0, GPU 222 (MiB)\n",
      "[05/31/2022-13:22:51] [TRT] [I] The logger passed into createInferRuntime differs from one already provided for an existing builder, runtime, or refitter. Uses of the global logger, returned by nvinfer1::getLogger(), will return the existing value.\n",
      "\n",
      "[05/31/2022-13:22:51] [TRT] [I] [MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 2350, GPU 7350 (MiB)\n",
      "[05/31/2022-13:22:51] [TRT] [I] Loaded engine size: 15 MiB\n",
      "[05/31/2022-13:22:51] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +1, GPU +8, now: CPU 2367, GPU 7374 (MiB)\n",
      "[05/31/2022-13:22:51] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +0, GPU +10, now: CPU 2367, GPU 7384 (MiB)\n",
      "[05/31/2022-13:22:51] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +15, now: CPU 0, GPU 237 (MiB)\n",
      "[05/31/2022-13:22:51] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 2351, GPU 7376 (MiB)\n",
      "[05/31/2022-13:22:51] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +0, GPU +8, now: CPU 2351, GPU 7384 (MiB)\n",
      "[05/31/2022-13:22:51] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +3, now: CPU 0, GPU 240 (MiB)\n",
      "[05/31/2022-13:22:51] [TRT] [I] The logger passed into createInferRuntime differs from one already provided for an existing builder, runtime, or refitter. Uses of the global logger, returned by nvinfer1::getLogger(), will return the existing value.\n",
      "\n",
      "[05/31/2022-13:22:51] [TRT] [I] [MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 2354, GPU 7392 (MiB)\n",
      "[05/31/2022-13:22:51] [TRT] [I] Loaded engine size: 15 MiB\n",
      "[05/31/2022-13:22:51] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 2370, GPU 7416 (MiB)\n",
      "[05/31/2022-13:22:51] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +0, GPU +8, now: CPU 2370, GPU 7424 (MiB)\n",
      "[05/31/2022-13:22:51] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +15, now: CPU 0, GPU 255 (MiB)\n",
      "[05/31/2022-13:22:51] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 2355, GPU 7416 (MiB)\n",
      "[05/31/2022-13:22:51] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +0, GPU +8, now: CPU 2355, GPU 7424 (MiB)\n",
      "[05/31/2022-13:22:52] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +3, now: CPU 0, GPU 258 (MiB)\n",
      "[05/31/2022-13:22:52] [TRT] [I] The logger passed into createInferRuntime differs from one already provided for an existing builder, runtime, or refitter. Uses of the global logger, returned by nvinfer1::getLogger(), will return the existing value.\n",
      "\n",
      "[05/31/2022-13:22:52] [TRT] [I] [MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 2358, GPU 7432 (MiB)\n",
      "[05/31/2022-13:22:52] [TRT] [I] Loaded engine size: 15 MiB\n",
      "[05/31/2022-13:22:52] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 2374, GPU 7456 (MiB)\n",
      "[05/31/2022-13:22:52] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +0, GPU +8, now: CPU 2374, GPU 7464 (MiB)\n",
      "[05/31/2022-13:22:52] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +15, now: CPU 0, GPU 273 (MiB)\n",
      "[05/31/2022-13:22:52] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 2359, GPU 7456 (MiB)\n",
      "[05/31/2022-13:22:52] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +0, GPU +8, now: CPU 2359, GPU 7464 (MiB)\n",
      "[05/31/2022-13:22:52] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +3, now: CPU 0, GPU 276 (MiB)\n",
      "[05/31/2022-13:22:52] [TRT] [I] The logger passed into createInferRuntime differs from one already provided for an existing builder, runtime, or refitter. Uses of the global logger, returned by nvinfer1::getLogger(), will return the existing value.\n",
      "\n",
      "[05/31/2022-13:22:52] [TRT] [I] [MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 2361, GPU 7472 (MiB)\n",
      "[05/31/2022-13:22:52] [TRT] [I] Loaded engine size: 15 MiB\n",
      "[05/31/2022-13:22:52] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 2377, GPU 7496 (MiB)\n",
      "[05/31/2022-13:22:52] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +1, GPU +8, now: CPU 2378, GPU 7504 (MiB)\n",
      "[05/31/2022-13:22:52] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +15, now: CPU 0, GPU 291 (MiB)\n",
      "[05/31/2022-13:22:52] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 2362, GPU 7496 (MiB)\n",
      "[05/31/2022-13:22:52] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +0, GPU +8, now: CPU 2362, GPU 7504 (MiB)\n",
      "[05/31/2022-13:22:52] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +3, now: CPU 0, GPU 294 (MiB)\n",
      "[05/31/2022-13:22:52] [TRT] [I] The logger passed into createInferRuntime differs from one already provided for an existing builder, runtime, or refitter. Uses of the global logger, returned by nvinfer1::getLogger(), will return the existing value.\n",
      "\n",
      "[05/31/2022-13:22:52] [TRT] [I] [MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 2364, GPU 7510 (MiB)\n",
      "[05/31/2022-13:22:52] [TRT] [I] Loaded engine size: 15 MiB\n",
      "[05/31/2022-13:22:52] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 2381, GPU 7536 (MiB)\n",
      "[05/31/2022-13:22:52] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +0, GPU +8, now: CPU 2381, GPU 7544 (MiB)\n",
      "[05/31/2022-13:22:52] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +15, now: CPU 0, GPU 309 (MiB)\n",
      "[05/31/2022-13:22:52] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 2365, GPU 7536 (MiB)\n",
      "[05/31/2022-13:22:52] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +0, GPU +8, now: CPU 2365, GPU 7544 (MiB)\n",
      "[05/31/2022-13:22:52] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +3, now: CPU 0, GPU 312 (MiB)\n",
      "[05/31/2022-13:22:52] [TRT] [I] The logger passed into createInferRuntime differs from one already provided for an existing builder, runtime, or refitter. Uses of the global logger, returned by nvinfer1::getLogger(), will return the existing value.\n",
      "\n",
      "[05/31/2022-13:22:52] [TRT] [I] [MemUsageChange] Init CUDA: CPU +0, GPU +0, now: CPU 2368, GPU 7550 (MiB)\n",
      "[05/31/2022-13:22:52] [TRT] [I] Loaded engine size: 15 MiB\n",
      "[05/31/2022-13:22:52] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +1, GPU +8, now: CPU 2385, GPU 7574 (MiB)\n",
      "[05/31/2022-13:22:52] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +0, GPU +10, now: CPU 2385, GPU 7584 (MiB)\n",
      "[05/31/2022-13:22:52] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in engine deserialization: CPU +0, GPU +15, now: CPU 0, GPU 327 (MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[05/31/2022-13:22:52] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +8, now: CPU 2369, GPU 7576 (MiB)\n",
      "[05/31/2022-13:22:52] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +0, GPU +8, now: CPU 2369, GPU 7584 (MiB)\n",
      "[05/31/2022-13:22:52] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +3, now: CPU 0, GPU 330 (MiB)\n"
     ]
    }
   ],
   "source": [
    "number_plate_detection_and_reading_trt_runtime = NumberPlateDetectionAndReadingTrtRuntime(\n",
    "    \"number_plate_detection_and_reading_runtime\", \n",
    "    image_loader=\"opencv\", # Try 'turbo' for faster performance.\n",
    "    \n",
    "    # numberplate detector trt paths\n",
    "    path_to_model=os.path.join(nomeroff_net_dir,\n",
    "                               \"./data/model_repository/yolov5s/1/model.engine\"),\n",
    "    # numberplate classification trt paths\n",
    "    path_to_classification_model=os.path.join(nomeroff_net_dir,\n",
    "                                              \"./data/model_repository/numberplate_options/1/model.trt\"),\n",
    "    options = {\n",
    "        \"class_region\": [\n",
    "                \"military\",\n",
    "                \"eu_ua_2015\",\n",
    "                \"eu_ua_2004\",\n",
    "                \"eu_ua_1995\",\n",
    "                \"eu\",\n",
    "                \"xx_transit\",\n",
    "                \"ru\",\n",
    "                \"kz\",\n",
    "                \"eu-ua-fake-dpr\",\n",
    "                \"eu-ua-fake-lpr\",\n",
    "                \"ge\",\n",
    "                \"by\",\n",
    "                \"su\",\n",
    "                \"kg\",\n",
    "                \"am\"\n",
    "            ],\n",
    "            \"count_lines\": [\n",
    "                1,\n",
    "                2,\n",
    "                3\n",
    "            ],\n",
    "    },\n",
    "    \n",
    "    # numberplate text recognition trt paths\n",
    "    prisets={\n",
    "        \"eu_ua_2004_2015\": {\n",
    "            \"for_regions\": [\"eu_ua_2015\", \"eu_ua_2004\"],\n",
    "            \"model_path\": os.path.join(nomeroff_net_dir, \n",
    "                                       \"./data/model_repository/ocr-eu_ua_2004_2015/1/model.trt\")\n",
    "        },\n",
    "        \"eu_ua_1995\": {\n",
    "            \"for_regions\": [\"eu_ua_1995\"],\n",
    "            \"model_path\": os.path.join(nomeroff_net_dir,\n",
    "                                       \"./data/model_repository/ocr-eu_ua_1995/1/model.trt\")\n",
    "        },\n",
    "        \"eu\": {\n",
    "            \"for_regions\": [\"eu\"],\n",
    "            \"model_path\": os.path.join(nomeroff_net_dir,\n",
    "                                       \"./data/model_repository/ocr-eu/1/model.trt\")\n",
    "        },\n",
    "        \"ru\": {\n",
    "            \"for_regions\": [\"ru\", \"eu-ua-ordlo-lpr\", \"eu-ua-ordlo-dpr\"],\n",
    "            \"model_path\": os.path.join(nomeroff_net_dir,\n",
    "                                       \"./data/model_repository/ocr-ru/1/model.trt\")\n",
    "        },\n",
    "        \"kz\": {\n",
    "            \"for_regions\": [\"kz\"],\n",
    "            \"model_path\": os.path.join(nomeroff_net_dir,\n",
    "                                       \"./data/model_repository/ocr-kz/1/model.trt\")\n",
    "        },\n",
    "        \"ge\": {\n",
    "            \"for_regions\": [\"ge\"],\n",
    "            \"model_path\": os.path.join(nomeroff_net_dir,\n",
    "                                       \"./data/model_repository/ocr-ge/1/model.trt\")\n",
    "        },\n",
    "        \"su\": {\n",
    "            \"for_regions\": [\"su\"],\n",
    "            \"model_path\": os.path.join(nomeroff_net_dir,\n",
    "                                       \"./data/model_repository/ocr-su/1/model.trt\")\n",
    "        }\n",
    "    },\n",
    "    default_label = \"eu\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pass 0\n",
      "[05/31/2022-13:22:52] [TRT] [I] [MemUsageChange] Init cuBLAS/cuBLASLt: CPU +0, GPU +10, now: CPU 2884, GPU 7893 (MiB)\n",
      "[05/31/2022-13:22:52] [TRT] [I] [MemUsageChange] Init cuDNN: CPU +0, GPU +10, now: CPU 2884, GPU 7903 (MiB)\n",
      "[05/31/2022-13:22:52] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +12, now: CPU 0, GPU 342 (MiB)\n"
     ]
    }
   ],
   "source": [
    "num_run = 1\n",
    "batch_size = 1\n",
    "num_workers = 1\n",
    "\n",
    "# change on ./data/examples/benchmark_oneline_np_images/*\n",
    "images = glob(os.path.join(nomeroff_net_dir, \"./data/examples/benchmark_oneline_np_images/1.jpeg\"))\n",
    "\n",
    "number_plate_detection_and_reading_trt_runtime.clear_stat()\n",
    "\n",
    "for i in range(num_run):\n",
    "    print(f\"pass {i}\")\n",
    "    outputs = number_plate_detection_and_reading_trt_runtime(\n",
    "        images, \n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "timer_stat = number_plate_detection_and_reading_trt_runtime.get_timer_stat(len(images)*num_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jetson Xavier Tensorrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1 photos\n",
      "One photo process 0.40032386779785156 seconds\n",
      "\n",
      "detect_bbox_time_all 0.013784408569335938 per one photo\n",
      "craft_time_all 0.3688802719116211 per one photo\n",
      "classification_time_all 0.003966808319091797 per one photo\n",
      "ocr_time_all 0.0023407936096191406 per one photo\n"
     ]
    }
   ],
   "source": [
    "print(f\"Processed {len(images)} photos\")\n",
    "print(f\"One photo process {timer_stat['NumberPlateDetectionAndReadingTrtRuntime.call']} seconds\")\n",
    "print()\n",
    "print(f\"detect_bbox_time_all {timer_stat['NumberPlateLocalizationTrt.call']} per one photo\")\n",
    "print(f\"craft_time_all {timer_stat['NumberPlateKeyPointsDetection.call']} per one photo\")\n",
    "print(f\"classification_time_all {timer_stat['NumberPlateClassificationTrt.call']} per one photo\")\n",
    "print(f\"ocr_time_all {timer_stat['NumberPlateTextReadingTrt.call']} per one photo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intel(R) Core(TM) i9-9900K CPU @ 3.60GHz + RTX 3090 Tensorrt in Docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
