{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly [Downloads models](https://github.com/spmallick/learnopencv/tree/master/FaceDetectionComparison/models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup detectron2 logger\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "# import some common libraries\n",
    "import numpy as np\n",
    "\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Detector:\n",
    "    def __init__(self, plugin_config = None):\n",
    "        if plugin_config is None:\n",
    "            plugin_config = {}\n",
    "        cfg = get_cfg()\n",
    "        # add project-specific config (e.g., TensorMask) here if you're not running a model in detectron2's core library\n",
    "        cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\"))\n",
    "        cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model\n",
    "        # Find a model from detectron2's model zoo. You can use the https://dl.fbaipublicfiles... url as well\n",
    "        cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
    "        self.predictor = DefaultPredictor(cfg)\n",
    "\n",
    "    def check(self, img):\n",
    "       outputs = self.predictor(img)\n",
    "       contours = []\n",
    "       clases   = []\n",
    "       bboxs    = []\n",
    "       scores   = []\n",
    "       for mask, cls, bbox, score in zip(outputs[\"instances\"].pred_masks, \\\n",
    "                                  outputs[\"instances\"].pred_classes, \\\n",
    "                                  outputs[\"instances\"].pred_boxes, \\\n",
    "                                  outputs[\"instances\"].scores):\n",
    "           cls   = int(cls.to(\"cpu\"))\n",
    "           score = float(score.to(\"cpu\"))\n",
    "           bbox  = [int(x) for x in bbox.to(\"cpu\")]\n",
    "           poly, hierarchy = cv2.findContours(\n",
    "               np.array(mask.to(\"cpu\")).astype(np.uint8),\n",
    "               cv2.RETR_TREE,\n",
    "               cv2.CHAIN_APPROX_SIMPLE\n",
    "               )\n",
    "           poly = np.array([cnt for cnt, h in zip(poly, hierarchy[0]) \\\n",
    "                           if ((len(np.unique(cnt, axis=0)) > 3) and (h[3] == -1))], dtype=object)\n",
    "           clases.append(cls)\n",
    "           contours.append(poly)\n",
    "           bboxs.append(bbox)\n",
    "           scores.append(score)\n",
    "       return clases, bboxs, contours, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (20,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading model...\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] loading model...\")\n",
    "net = cv2.dnn.readNetFromCaffe(\"deploy.prototxt\", \"res10_300x300_ssd_iter_140000_fp16.caffemodel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = Detector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_face(image, min_confidence=0.7):\n",
    "    (h, w) = image.shape[:2]\n",
    "    blob = cv2.dnn.blobFromImage(cv2.resize(person_img, (300, 300)), 1.0, (300, 300), (104.0, 177.0, 123.0))\n",
    "    net.setInput(blob)\n",
    "    detections = net.forward()\n",
    "    detected_faces = []\n",
    "    # loop over the detections\n",
    "    for i in range(0, detections.shape[2]):\n",
    "        # extract the confidence (i.e., probability) associated with the\n",
    "        # prediction\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "        # filter out weak detections by ensuring the `confidence` is\n",
    "        # greater than the minimum confidence\n",
    "        if confidence > min_confidence:\n",
    "            # compute the (x, y)-coordinates of the bounding box for the\n",
    "            # object\n",
    "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "            detected_faces.append([startX, startY, endX, endY])\n",
    "    return image, detected_faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_rect_bbox(bboxs_item, w, h):\n",
    "    bw = bboxs_item[2]-bboxs_item[0]\n",
    "    bh = bboxs_item[3]-bboxs_item[1]\n",
    "    \n",
    "    y1 = bboxs_item[1]\n",
    "    y2 = bboxs_item[3]\n",
    "    x1 = bboxs_item[0]\n",
    "    x2 = bboxs_item[2]\n",
    "        \n",
    "    if bw > bh:\n",
    "        diff = (bw-bh)/2\n",
    "        y1 -= diff\n",
    "        y2 += diff\n",
    "        if y1 < 0:\n",
    "            y2 -=  y1\n",
    "            y1 = 0\n",
    "        if y2 > h:\n",
    "            y1 -= h-y2\n",
    "            y2 = h\n",
    "        if y1 < 0:\n",
    "            y1 = 0\n",
    "    if bw < bh:\n",
    "        diff = (bh-bw)/2\n",
    "        x1 -= diff\n",
    "        x2 += diff\n",
    "        if x1 < 0:\n",
    "            x2 -=  x1\n",
    "            x1 = 0\n",
    "        if x2 > w:\n",
    "            x1 -= w-x2\n",
    "            x2 = w\n",
    "        if x1 < 0:\n",
    "            x1 = 0\n",
    "    return int(x1), int(x2), int(y1), int(y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anonymize_face_pixelate(image, blocks=5):\n",
    "    # divide the input image into NxN blocks\n",
    "    (h, w) = image.shape[:2]\n",
    "    xSteps = np.linspace(0, w, blocks + 1, dtype=\"int\")\n",
    "    ySteps = np.linspace(0, h, blocks + 1, dtype=\"int\")\n",
    "    # loop over the blocks in both the x and y direction\n",
    "    for i in range(1, len(ySteps)):\n",
    "        for j in range(1, len(xSteps)):\n",
    "            # compute the starting and ending (x, y)-coordinates\n",
    "            # for the current block\n",
    "            startX = xSteps[j - 1]\n",
    "            startY = ySteps[i - 1]\n",
    "            endX = xSteps[j]\n",
    "            endY = ySteps[i]\n",
    "            # extract the ROI using NumPy array slicing, compute the\n",
    "            # mean of the ROI, and then draw a rectangle with the\n",
    "            # mean RGB values over the ROI in the original image\n",
    "            roi = image[startY:endY, startX:endX]\n",
    "            (B, G, R) = [int(x) for x in cv2.mean(roi)[:3]]\n",
    "            cv2.rectangle(image, (startX, startY), (endX, endY),\n",
    "                (B, G, R), -1)\n",
    "    # return the pixelated blurred image\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import tqdm\n",
    "\n",
    "debug = 1\n",
    "all_photos = 0\n",
    "with_people = 0\n",
    "g = glob.glob(\"/var/www/nomeroff-net/yolov5/data_source/many_line/*\")\n",
    "for img_path in tqdm.tqdm(g):\n",
    "    try:\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            raise Exception(\"None\")\n",
    "    except:\n",
    "        continue\n",
    "    h, w, c = img.shape\n",
    "    clases, bboxs, contours, scores = detector.check(img)\n",
    "    pretty_clases = []\n",
    "    for clases_item, bboxs_item, contours_item, scores_item in zip(clases, bboxs, contours, scores):\n",
    "        bw = bboxs_item[2]-bboxs_item[0]\n",
    "        bh = bboxs_item[3]-bboxs_item[1]\n",
    "        x1, x2, y1, y2 = to_rect_bbox(bboxs_item, w, h)\n",
    "        area = (bw*bh)/(w*h)\n",
    "        if scores_item > 0.7 and clases_item==0:\n",
    "            person_img = img[y1:y2, x1:x2]\n",
    "            person_img, detected_faces = detect_face(person_img)\n",
    "            for face_bbox in detected_faces:\n",
    "                face_x1, face_y1, face_x2, face_y2 = face_bbox\n",
    "                face_x1 += x1\n",
    "                face_y1 += y1\n",
    "                face_x2 += x1\n",
    "                face_y2 += y1\n",
    "                img[face_y1:face_y2, face_x1:face_x2] = anonymize_face_pixelate(\n",
    "                    img[face_y1:face_y2, face_x1:face_x2]\n",
    "                )\n",
    "            if len(detected_faces):\n",
    "                pretty_clases.append(clases_item)\n",
    "    if 0 in pretty_clases:\n",
    "        with_people += 1\n",
    "        if debug:\n",
    "            plt.imshow(img[:,:,::-1])\n",
    "            plt.show()\n",
    "        else:\n",
    "            img = cv2.imwrite(img_path, img)\n",
    "    all_photos += 1\n",
    "    #break\n",
    "print(\"all_photos\", all_photos)\n",
    "print(\"with_people\", with_people)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}